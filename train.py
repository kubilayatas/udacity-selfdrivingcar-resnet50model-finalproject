# -*- coding: utf-8 -*-
"""UdacitySelfDrive-ResNet50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y_ID1rG8xryrV1wVbdVFFuO8BpvrccfQ
"""

from google.colab import drive
drive.mount('/content/gdrive')

import keras
from keras.applications import resnet50
resnet=keras.applications.resnet50.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None)
from keras.layers import Dropout, Flatten, Dense

#########################################################
# Locking First 51 Layer #  Non Trainable
#########################################################
for layer in resnet.layers[:51]:
    layer.trainable = False

#########################################################
# Creating Model
#########################################################
top_model = keras.Sequential()
top_model.add(resnet)
top_model.add(Dense(512,activation="relu"))
top_model.add(Dense(256,activation="relu"))
top_model.add(Dense(64,activation="relu"))
top_model.add(Dense(1,activation="tanh"))

#########################################################
# Load Weights From early trained models if you want
#########################################################
#checkpoint_folder="gdrive/My Drive/Self-Driving-Car-Data/CheckPointModel/"
#top_model.load_weights(checkpoint_folder+"saved-model-{epoch:02d}-{val_acc:.2f}.hdf5")

#########################################################
# Defining Root Mean Squared Error Function
#########################################################
from keras import backend as K
def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true)))

top_model.compile(optimizer = "adam", loss = root_mean_squared_error, 
              metrics =["accuracy"])

#########################################################
# Print out Summary
#########################################################
print("ResNet50 Pre-trained Ağına ait summary list:")
print("=========================================================================")
print(resnet.summary())
print()
print()
print("Top-Model Ağına ait summary list:")
print("=========================================================================")
print(top_model.summary())

import os
import pandas as pd
import numpy as np
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers

#########################################################
# Importing Data and Concatinate
#########################################################
path="gdrive/My Drive/Self-Driving-Car-Data/"
##
filename_read=os.path.join(path+"HMB_1/","interpolated.csv")
hmb1=pd.read_csv(filename_read)
hmb1["filename"]=path+"HMB_1/"+hmb1["filename"]
##
filename_read=os.path.join(path+"HMB_2/","interpolated.csv")
hmb2=pd.read_csv(filename_read)
hmb2["filename"]=path+"HMB_2/"+hmb2["filename"]
##
filename_read=os.path.join(path+"HMB_4/","interpolated.csv")
hmb4=pd.read_csv(filename_read)
hmb4["filename"]=path+"HMB_4/"+hmb4["filename"]
##
filename_read=os.path.join(path+"HMB_5/","interpolated.csv")
hmb5=pd.read_csv(filename_read)
hmb5["filename"]=path+"HMB_5/"+hmb5["filename"]
##
filename_read=os.path.join(path+"HMB_6/","interpolated.csv")
hmb6=pd.read_csv(filename_read)
hmb6["filename"]=path+"HMB_6/"+hmb6["filename"]
##
SelfDriveData=pd.concat([hmb1,hmb2,hmb4,hmb5,hmb6], keys=["hmb1","hmb2","hmb4","hmb5","hmb6"])
##

#########################################################
# Shuffle Shfufle Suhffle Shulffle Shufefl Shufelf
#########################################################
SelfDriveData=SelfDriveData.reindex(np.random.permutation(SelfDriveData.index))

#########################################################
# %80-%20 Training-Validation rate
#########################################################
mask=np.random.rand(len(SelfDriveData))<0.8
TrainDataframe=pd.DataFrame(SelfDriveData[mask])
ValidDataframe=pd.DataFrame(SelfDriveData[~mask])

nb_train_samples = TrainDataframe.shape[0]+1
nb_validation_samples = ValidDataframe.shape[0]+1
epochs     = 32
batch_size = 32
img_height = 224
img_width  = 224

#########################################################
# Data Augmentation
#########################################################
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.0,
    horizontal_flip=False,
    vertical_flip=False)

test_datagen = ImageDataGenerator(rescale=1. / 255)
#########################################################
# Train Data Generation
#########################################################
train_generator = train_datagen.flow_from_dataframe(
    TrainDataframe, 
    directory=None, 
    x_col='filename', 
    y_col='angle', 
    target_size=(224, 224), 
    color_mode='rgb', 
    classes=None, 
    class_mode='other', 
    batch_size=batch_size, 
    shuffle=True, 
    seed=None, 
    save_to_dir=None, 
    save_prefix='', 
    save_format='png', 
    subset=None, 
    interpolation='nearest', 
    drop_duplicates=True)

#########################################################
# Validation Data Generation
#########################################################
validation_generator = test_datagen.flow_from_dataframe(
    ValidDataframe, 
    directory=None, 
    x_col='filename', 
    y_col='angle', 
    target_size=(224, 224), 
    color_mode='rgb', 
    classes=None, 
    class_mode='other', 
    batch_size=batch_size, 
    shuffle=True, 
    seed=None, 
    save_to_dir=None, 
    save_prefix='', 
    save_format='png', 
    subset=None, 
    interpolation='nearest', 
    drop_duplicates=True)

#########################################################
# CheckPoint of Trained Models
#########################################################
from keras.callbacks import ModelCheckpoint
path_checkpoint="gdrive/My Drive/Self-Driving-Car-Data/CheckPointModel/"
filepath = path_checkpoint+"saved-model-{epoch:02d}-{val_acc:.2f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=2, save_best_only=False, mode='max')

#########################################################
# Fitting Model
#########################################################
import math
top_model.fit_generator(
    train_generator, 
    steps_per_epoch=math.ceil(nb_train_samples / batch_size), 
    epochs=epochs, 
    verbose=2, 
    callbacks=None, 
    validation_data=validation_generator, 
    validation_steps=math.ceil(nb_validation_samples / batch_size), 
    use_multiprocessing=True, 
    shuffle=True, 
    initial_epoch=0)
